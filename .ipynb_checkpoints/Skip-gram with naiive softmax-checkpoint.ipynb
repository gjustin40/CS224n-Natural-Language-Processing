{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Skip-gram with naiive softmax\n",
    "- Implementation model 'Skip-gram with naiive softmax'\n",
    "- https://nbviewer.jupyter.org/github/DSKSD/DeepNLP-models-Pytorch/blob/master/notebooks/01.Skip-gram-Naive-Softmax.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # torch\n",
    "import torch.nn as nn  # neural network\n",
    "from torch.autograd import Variable  # variable function(grad 계산 가능하게)\n",
    "import torch.optim as optim  # optimizer\n",
    "import torch.nn.functional as F  # 뭔지 모름\n",
    "import nltk  # Natural Language Processing Toolkit\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# 일자로 펴주기\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "random.seed(1024)  # seec값 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version :  0.4.1\n",
      "nltk version :  3.2.4\n"
     ]
    }
   ],
   "source": [
    "print('torch version : ', torch.__version__)\n",
    "print('nltk version : ', nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()  # Cuda 사용 가능 여부(GUP)\n",
    "gpus = [0]\n",
    "if USE_CUDA:\n",
    "    torch.cuda.set_device(gpus[0])\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatch(batch_size, train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex = 0\n",
    "    eindex = batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex:eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex + batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "\n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Word Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래 함수들을 직관적으로 바꾸기\n",
    "\n",
    "\n",
    "def prepare_sequence(seq,\n",
    "                     word2index):  # seqence(list 등)이 왔을 때 각 단어들을 인덱스(숫자)로 바꿔라\n",
    "    idxs = []\n",
    "    for w in seq:\n",
    "        if word2index.get(\n",
    "                w) != None:  # 우리가 만든 voca에 해당 단어가 있으면 해당 단어에 상응하는 인덱스로 바꿔라\n",
    "            idxs.append(word2index[w])\n",
    "        else:\n",
    "            idxs.append(\n",
    "                word2index[\"<UNK>\"])  # voca에 해당 단어가 없으면 '<UNK>'의 인덱스(0)로 바꿔라\n",
    "    return Variable(LongTensor(\n",
    "        idxs))  # 인덱스가 모인 list를 DataTpye이 LongTensor인 Tensor로 바꾸고 Variable을 씌어라\n",
    "\n",
    "\n",
    "def prepare_word(word, word2index):  # word가 들어올 때 해당 word를 인덱스로 바꿔라\n",
    "    if word2index.get(word) != None:  # 해당 word가 있다면 LongTensor로 변환하고 출력해라\n",
    "        return Variable(LongTensor(word2index[word]))\n",
    "    else:\n",
    "        return LongTensor(\n",
    "            [word2index['<UNK>']])  # 해당 word가 없다면 UNK인덱스(0)로 출력해라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"],seq))\n",
    "    return Variable(LongTensor(idxs))\n",
    "\n",
    "\n",
    "def prepare_word(word, word2index):\n",
    "    return Variable(LongTensor([word2index[word]]) if word2index.get(word) is not None else LongTensor([word2index[\"<UNK>\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data\n",
    "- Load corpus : **Gutenberg corpis**\n",
    "- gutenberg corpus가 없다면 **nltk.download()**를 통해 다운받을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 662,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids() # gutenberg 안에 있는 문서들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시험을 위해 문장 샘플링\n",
    "corpus = list(nltk.corpus.gutenberg.sents('melville-moby_dick.txt'))[:100] # 해당 문서를 문장으로 나누기(100개만)\n",
    "corpus = [[word.lower() for word in sent] for sent in corpus]  # 문장을 단어로 분해, 소문자 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 664,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stopword 뽑아내기\n",
    "word_count = Counter(flatten(corpus))  # 각각의 단어의 개수, sorted\n",
    "border = int(len(word_count) * 0.01)  # 단어 종류의 개수 * 0.01\n",
    "\n",
    "# 가장 많이 사용된 문자 상위 border개, 하위 border개\n",
    "stopwords = word_count.most_common()[:border] + list(reversed(word_count.most_common()))[:border]\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',', '.', 'the', 'of', 'and', 'man', 'artificial', 'civitas', '--(', 'state']"
      ]
     },
     "execution_count": 665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = [s[0] for s in stopwords]  # (word, number) 중 word만 추출\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build Voca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word in corpus :  592\n",
      "Number of word in vocab :  583\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set(flatten(corpus)) - set(stopwords))  # stopword 제거하기\n",
    "vocab.append('<UNK>')\n",
    "print('Number of word in corpus : ', len(set(flatten(corpus))))\n",
    "print('Number of word in vocab : ', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'with'"
      ]
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index = {'<UNK>': 0}  # '<UNK>'fmf 0 index로 만들어주기\n",
    "\n",
    "# 각 단어마다 index 할당해주기\n",
    "for vo in vocab:\n",
    "    if word2index.get(vo) is None:\n",
    "        word2index[vo] = len(word2index)\n",
    "        \n",
    "# index : word -> word : index로 변환하기\n",
    "index2word = {i: w for w, i in word2index.items()}\n",
    "index2word[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prepare Data\n",
    "- **nltk.ngrams(sequence, n, pad_lef = False, pad_right = False, pad_symbol = None)**\n",
    "- http://madhukaudantha.blogspot.com/2015/05/nltk-tutorial03-n-gram.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<DUMMY>', '<DUMMY>', '<DUMMY>', '[', 'moby', 'dick', 'by')"
      ]
     },
     "execution_count": 668,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_size = 3\n",
    "windows = flatten([list(nltk.ngrams(['<DUMMY>'] * window_size + c + ['<DUMMY>'] * window_size,window_size * 2 + 1)) for c in corpus])\n",
    "windows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[', 'moby'), ('[', 'dick'), ('[', 'by'), ('moby', '['), ('moby', 'dick'), ('moby', 'by')]\n"
     ]
    }
   ],
   "source": [
    "# My code\n",
    "train_data = []  # (center word, context word)\n",
    "\n",
    "for win in windows:\n",
    "    for i in range(window_size * 2 + 1):  # 수정된 부분\n",
    "        if win[i] == '<DUMMY>' or i == window_size:  # 수정된 부분\n",
    "            continue\n",
    "        train_data.append((win[window_size], win[i]))\n",
    "print(train_data[:window_size * 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train_data :  7606\n",
      "('[', 'moby')\n"
     ]
    }
   ],
   "source": [
    "print('Number of train_data : ', len(train_data))\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_p = []\n",
    "y_p = []\n",
    "\n",
    "for tr in train_data:\n",
    "    X_p.append(prepare_word(tr[0], word2index).view(1, -1)) # center word 2차 행렬로 만들기\n",
    "    y_p.append(prepare_word(tr[1], word2index).view(1, -1)) # context word 2차 행렬로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7606"
      ]
     },
     "execution_count": 672,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = list(zip(X_p, y_p)) # (tensor(conter word), tensor(context word))\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 8. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, projection_dim):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, projection_dim) # procjetion_dim의 개수로 벡터화([a,b,c,d])\n",
    "        self.embedding_u = nn.Embedding(vocab_size, projection_dim) # procjetion_dim의 개수로 벡터화([a,b,c,d])\n",
    "        \n",
    "        self.embedding_v.weight.data.uniform_(-1, 1) # -1 ~ 1 값으로 초기화\n",
    "        self.embedding_u.weight.data.uniform_(0, 0) # -1 ~ 1 값으로 초기화\n",
    "        #self.out = nn.Linear(procjection_dim, vocab_size) \n",
    "        \n",
    "    def forward(self, center_words, target_words, outer_words):\n",
    "        center_embeds = self.embedding_v(center_words) # (Batch x 1 x D) center_words 벡터화\n",
    "        target_embeds = self.embedding_u(target_words) # (Batch x 1 x D)\n",
    "        outer_embeds = self.embedding_u(outer_words) # (batch x V x D)\n",
    "        \n",
    "        scores = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2) # Bx1xD * BxDx1 -> Bx1\n",
    "        norm_scores = outer_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2) # BxVxD * BxDx1 -> BxV\n",
    "        \n",
    "        nll = -torch.mean(torch.log(torch.exp(scores)/torch.sum(torch.exp(norm_scores), 1).unsqueeze(1))) # log-softmax\n",
    "        \n",
    "        return nll\n",
    "    \n",
    "    def prediction(self, inputs):\n",
    "        embeds = self.embedding_v(inputs)\n",
    "        \n",
    "        return embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 30\n",
    "batch_size = 256\n",
    "epoch = 100\n",
    "\n",
    "losses = []\n",
    "model = Skipgram(len(word2index), embedding_size)\n",
    "if USE_CUDA:\n",
    "    model = model.CUDA()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, mean_loss ; 6.19\n",
      "Epoch : 10, mean_loss ; 4.38\n",
      "Epoch : 20, mean_loss ; 3.47\n",
      "Epoch : 30, mean_loss ; 3.31\n",
      "Epoch : 40, mean_loss ; 3.26\n",
      "Epoch : 50, mean_loss ; 3.24\n",
      "Epoch : 60, mean_loss ; 3.23\n",
      "Epoch : 70, mean_loss ; 3.21\n",
      "Epoch : 80, mean_loss ; 3.21\n",
      "Epoch : 90, mean_loss ; 3.20\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch):\n",
    "    for i, batch in enumerate(getBatch(batch_size, train_data)):\n",
    "        inputs, targets = zip(*batch)\n",
    "        \n",
    "        inputs = torch.cat(inputs) # B x 1\n",
    "        targets = torch.cat(targets) # B x 1\n",
    "        vocabs = prepare_sequence(list(vocab), word2index).expand(inputs.size(0), len(vocab))\n",
    "        model.zero_grad()\n",
    "        \n",
    "        loss = model(inputs, targets, vocabs)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.data.tolist())\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch : %d, mean_loss ; %.02f\" % (epoch, np.mean(losses)))\n",
    "        losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_similarity(target, word):\n",
    "    if USE_CUDA:\n",
    "        target_V = model.prediction(prepare_word(target, word2index))\n",
    "    else:\n",
    "        target_V = model.prediction(prepare_word(target, word2index))\n",
    "    \n",
    "    similarities = []\n",
    "    \n",
    "    for i in range(len(vocab)):\n",
    "        if vocab[i] == target:\n",
    "            continue\n",
    "        if USE_CUDA:\n",
    "            vector = model.prediction(prepare_word(list(vocab)[i], word2index))\n",
    "        else:\n",
    "            vector = model.prediction(prepare_word(list(vocab)[i], word2index))\n",
    "        \n",
    "        cosine_sim = F.cosine_similarity(target_V, vector).data.tolist()[0]\n",
    "        similarities.append([vocab[i], cosine_sim])\n",
    "    return sorted(similarities, key = lambda x: x[1], reverse = True)[:10] # sort by similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mockingly\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['embellished', 0.7563604116439819],\n",
       " ['handkerchief', 0.6995153427124023],\n",
       " ['gay', 0.6726282238960266],\n",
       " ['tail', 0.6395581960678101],\n",
       " ['queer', 0.6295337677001953],\n",
       " ['sore', 0.6193282008171082],\n",
       " ['eyes', 0.5807200074195862],\n",
       " ['known', 0.5707562565803528],\n",
       " ['incontinently', 0.5502141118049622],\n",
       " ['t', 0.5393688678741455]]"
      ]
     },
     "execution_count": 702,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = random.choice(list(vocab))\n",
    "print(test)\n",
    "word_similarity(test, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 새롭게 알게 된 내용\n",
    "### 1. 두 list의 차집합\n",
    "- 그냥 set(list1) - set(list2)를 빼면 된다.\n",
    "- 단, 더 많은 list에서 빼야 확인 가능!\n",
    "\n",
    "### 2. zip(*list)\n",
    "- 두 list가 한 list에 있을 때 따로 zip하는거랑 같은 효과\n",
    "https://stackoverflow.com/questions/29139350/difference-between-- ziplist-and-ziplist/29139418\n",
    "\n",
    "### 3. Dimension의 개념\n",
    "- 내가 평소에 쓰던 pandas의 axis와 같은 개념(0 : 행, 1 : 열)\n",
    "\n",
    "### 4. iterable의 개념\n",
    "- member를 하나씩 차례로 반환 가능한 object를 말한다.\n",
    "- iterable의 예로는 sequence type인 list, str, tuple 등이 대표적이다.\n",
    "- http://bluese05.tistory.com/55\n",
    "\n",
    "### 5. Python의 map()\n",
    "- built-in 함수로 list 나 dictionary와 같은 iterable 한 데이터를 인자로 받아 list 안의 개별 item을 함수의 인자로 전달하여<br>\n",
    "결과를 list 형태로 반환해 주는 함수이다.\n",
    "- map(str, [1, 2, 3])와 같이 자료형 int, float, str 등을 넣었는데 plus_ten처럼 함수(클래스)를 직접 만들어서 넣어도 됩니다.\n",
    "- http://bluese05.tistory.com/58\n",
    "\n",
    "### 6. Python의 lambda로 함수 만들기\n",
    "- https://dojang.io/mod/page/view.php?id=1059"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용한 PyTorch 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. torch.bmm\n",
    "![torch.bmm](image/torch.bmm.png)\n",
    "\n",
    "### 2. nn.embedding\n",
    "![nn.embedding](image/nn.embedding.png)\n",
    "\n",
    "### 3. torch.cat\n",
    "![torch.cat](image/torch.cat.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
